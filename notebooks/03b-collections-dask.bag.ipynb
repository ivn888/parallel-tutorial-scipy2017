{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel Collections\n",
    "---------------------\n",
    "\n",
    "Systems like Spark and Dask include \"big data\" collections with a small set of high-level primitives like `map`, `filter`, `groupby`, and `join`.  With these common patterns we can often handle computations that are more complex than map, but are still structured.\n",
    "\n",
    "In this section we repeat the submit example using the PySpark and the Dask.Bag APIs, which both provide parallel operations on linear collections of arbitrary objects.\n",
    "\n",
    "\n",
    "### Objectives\n",
    "\n",
    "*  Use high-level `pyspark` and `dask.bag` to parallelize common non-map patterns\n",
    "\n",
    "### Requirements\n",
    "\n",
    "*  Dask.bag\n",
    "\n",
    "*Note: the following exercises were designed to work with `dask 0.10.1`, you can check your installed version of `dask` with the following code*:\n",
    "\n",
    "```\n",
    "import dask\n",
    "print(dask.__version__)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "We again start with the following sequential code\n",
    "\n",
    "```python\n",
    "series = {}\n",
    "for fn in filenames:   # Simple map over filenames\n",
    "    series[fn] = pd.read_hdf(fn)['x']\n",
    "\n",
    "results = {}\n",
    "\n",
    "for a in filenames:    # Doubly nested loop over the same collection\n",
    "    for b in filenames:  \n",
    "        if a != b:     # Filter out bad elements\n",
    "            results[a, b] = series[a].corr(series[b])  # Apply function\n",
    "\n",
    "((a, b), corr) = max(results.items(), key=lambda kv: kv[1])  # Reduction\n",
    "```\n",
    "\n",
    "### Spark/Dask.bag methods\n",
    "\n",
    "We can construct most of the above computation with the following Spark/Dask.bag methods:\n",
    "\n",
    "*  `collection.map(function)`: apply function to each element in collection\n",
    "*  `collection.product(collection)`: Create new collection with every pair of inputs\n",
    "*  `collection.filter(predicate)`: Keep only elements of colleciton that match the predicate function\n",
    "*  `collection.max()`: Compute maximum element\n",
    "\n",
    "We use these briefly in isolated exercises and then combine them to rewrite the previous computation from the `submit` section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask.bag: Example API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.bag<from_se..., npartitions=5>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = db.from_sequence(range(5))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.compute()  # Gather results back to local process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Square each element\n",
    "\n",
    "b.map(lambda x: x ** 2).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Square each element and collect results\n",
    "\n",
    "b.map(lambda x: x ** 2).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only the even elements\n",
    "\n",
    "b.filter(lambda x: x % 2 == 0).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 4),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 3),\n",
       " (2, 4),\n",
       " (3, 0),\n",
       " (3, 1),\n",
       " (3, 2),\n",
       " (3, 3),\n",
       " (3, 4),\n",
       " (4, 0),\n",
       " (4, 1),\n",
       " (4, 2),\n",
       " (4, 3),\n",
       " (4, 4)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cartesian product of each pair of elements in two sequences (or the same sequence in this case)\n",
    "\n",
    "b.product(b).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (4, 0),\n",
       " (4, 1),\n",
       " (4, 2),\n",
       " (4, 3),\n",
       " (4, 4),\n",
       " (16, 0),\n",
       " (16, 1),\n",
       " (16, 2),\n",
       " (16, 3),\n",
       " (16, 4)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain operations to construct more complex computations\n",
    "\n",
    "(b.map(lambda x: x ** 2)\n",
    "  .product(b)\n",
    "  .filter(lambda tup: tup[0] % 2 == 0)\n",
    "  .compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Parallelize pairwise correlations with Dask.bag\n",
    "\n",
    "To make this a bit easier we're just going to compute the maximum correlation and not try to keep track of the stocks that yielded this maximal result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/json/aet.h5',\n",
       " '../data/json/afl.h5',\n",
       " '../data/json/aig.h5',\n",
       " '../data/json/al.h5',\n",
       " '../data/json/amgn.h5']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "filenames = sorted(glob(os.path.join('..', 'data', 'json', '*.h5')))  # ../data/json/*.json\n",
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.941803698004\n",
      "CPU times: user 2.66 s, sys: 164 ms, total: 2.83 s\n",
      "Wall time: 2.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Sequential Code\n",
    "\n",
    "series = []\n",
    "for fn in filenames:   # Simple map over filenames\n",
    "    series.append(pd.read_hdf(fn)['close'])\n",
    "\n",
    "results = []\n",
    "\n",
    "for a in series:    # Doubly nested loop over the same collection\n",
    "    for b in series:  \n",
    "        if not (a == b).all():     # Filter out comparisons of the same series \n",
    "            results.append(a.corr(b))  # Apply function\n",
    "\n",
    "result = max(results)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.941803698004\n",
      "CPU times: user 6.1 s, sys: 2.37 s, total: 8.47 s\n",
      "Wall time: 9.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Parallel code\n",
    "\n",
    "b = db.from_sequence(filenames)\n",
    "data = b.map(lambda fn: pd.read_hdf(fn)['close'])\n",
    "\n",
    "result = (data.product(data)\n",
    "          .filter(lambda d: not (d[0] == d[1]).all())\n",
    "          .map(lambda d: d[0].corr(d[1]))\n",
    "          .max()\n",
    "          .compute())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94180369800361052"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/collections-2.py\n",
    "### Parallel Code\n",
    "\n",
    "import dask.bag as db\n",
    "\n",
    "b = db.from_sequence(filenames)\n",
    "series = b.map(lambda fn: pd.read_hdf(fn)['close'])\n",
    "\n",
    "corr = (series.product(series)\n",
    "              .filter(lambda ab: not (ab[0] == ab[1]).all())\n",
    "              .map(lambda ab: ab[0].corr(ab[1]))\n",
    "              .max())\n",
    "\n",
    "result = corr.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import dask\n",
    "\n",
    "result = corr.compute(get=dask.local.get_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark solution for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/collections-1.py\n",
    "### Parallel code\n",
    "\n",
    "rdd = sc.parallelize(filenames)\n",
    "series = rdd.map(lambda fn: pd.read_hdf(fn)['close'])\n",
    "\n",
    "corr = (series.cartesian(series)\n",
    "              .filter(lambda ab: not (ab[0] == ab[1]).all())\n",
    "              .map(lambda ab: ab[0].corr(ab[1]))\n",
    "              .max())\n",
    "\n",
    "result = corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "*  Higher level collections include functions for common patterns\n",
    "*  Move data to collection, construct lazy computation, trigger at the end\n",
    "*  Used PySpark (`cartesian + map`) and Dask.bag (`product + map`) to handle nested for loop"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
